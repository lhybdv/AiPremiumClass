{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "\n",
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor([[1,2],[3,4]], dtype=torch.float32)\n",
    "print(data)\n",
    "print()\n",
    "\n",
    "data = torch.tensor([[1,2],[3,4]], dtype=torch.int32)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n",
      "2.4.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5],\n",
       "        [6, 7, 8]], dtype=torch.int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np_array = np.array(range(9)).reshape(3,3)\n",
    "data2 = torch.from_numpy(np_array)\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9843, 0.7961, 0.4247],\n",
       "        [0.2183, 0.1842, 0.7360],\n",
       "        [0.7865, 0.4933, 0.2088]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过已知张量维度，创建新随机张量\n",
    "data3 = torch.rand_like(data2, dtype=torch.float)\n",
    "data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.9118, 0.1711, 0.8242, 0.9695],\n",
      "        [0.1619, 0.9684, 0.6180, 0.1874],\n",
      "        [0.6997, 0.7820, 0.0972, 0.3703],\n",
      "        [0.2678, 0.7012, 0.3329, 0.9226]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (4,4)\n",
    "rand_tensor = torch.rand(shape) #随机张量\n",
    "ones_tensor = torch.ones(shape) #全为1\n",
    "zeros_tensor = torch.zeros(shape) #全为0\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "tensor([[0.4624, 0.0881, 0.6554],\n",
      "        [0.9020, 0.2808, 0.8022],\n",
      "        [0.8236, 0.6049, 0.0307],\n",
      "        [0.5206, 0.6282, 0.4797],\n",
      "        [0.6381, 0.2432, 0.1014]])\n",
      "tensor([[-0.0700,  1.5548, -2.4374],\n",
      "        [-0.3357, -1.5111,  1.2640],\n",
      "        [ 0.8685, -0.7651,  0.4317],\n",
      "        [ 0.3604,  1.3296,  1.2360],\n",
      "        [ 0.4609,  0.6390,  1.6991]])\n",
      "tensor([[ 0.3395, -1.3210,  0.7091],\n",
      "        [-0.5748, -1.2705, -0.2666],\n",
      "        [-0.9621,  1.7797, -0.8900],\n",
      "        [-0.4303,  0.6655, -1.2343],\n",
      "        [ 2.3350, -1.0902, -0.4905]])\n",
      "tensor([ 1.0000,  1.4737,  1.9474,  2.4211,  2.8947,  3.3684,  3.8421,  4.3158,\n",
      "         4.7895,  5.2632,  5.7368,  6.2105,  6.6842,  7.1579,  7.6316,  8.1053,\n",
      "         8.5789,  9.0526,  9.5263, 10.0000])\n"
     ]
    }
   ],
   "source": [
    "# 基于现有tensor构建，但使用新值填充\n",
    "m = torch.ones(5,3, dtype=torch.double)\n",
    "n = torch.rand_like(m, dtype=torch.float)\n",
    "\n",
    "# 获取tensor的大小\n",
    "print(m.size()) # torch.Size([5,3])\n",
    "\n",
    "# 均匀分布\n",
    "print(torch.rand(5,3))\n",
    "# 标准正态分布\n",
    "print(torch.randn(5,3))\n",
    "# 离散正态分布\n",
    "print(torch.normal(mean=.0,std=1.0,size=(5,3)))\n",
    "# 线性间隔向量(返回一个1维张量，包含在区间start和end上均匀间隔的steps个点)\n",
    "print(torch.linspace(start=1,end=10,steps=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([5, 2])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(5,2)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "tensor([[0.6222, 0.4525],\n",
      "        [0.3788, 0.3858],\n",
      "        [0.6362, 0.3520],\n",
      "        [0.5430, 0.8331],\n",
      "        [0.0353, 0.0178]])\n",
      "cpu\n",
      "tensor([[0.6222, 0.4525],\n",
      "        [0.3788, 0.3858],\n",
      "        [0.6362, 0.3520],\n",
      "        [0.5430, 0.8331],\n",
      "        [0.0353, 0.0178]])\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "# 检查pytorch是否支持GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    tensor = tensor.to(device)\n",
    "\n",
    "print(tensor)\n",
    "print(tensor.device)\n",
    "\n",
    "# mac上没有GPU，使用M系列芯片\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    tensor = tensor.to(device)\n",
    "\n",
    "print(tensor)\n",
    "print(tensor.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "First row:  tensor([1, 2, 3])\n",
      "First column:  tensor([1, 4, 7])\n",
      "Last column: tensor([3, 6, 9])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 2, 6],\n",
      "        [7, 2, 9]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor(range(1,10)).reshape(3,3)\n",
    "print(tensor)\n",
    "print('First row: ', tensor[0])\n",
    "print('First column: ', tensor[:, 0])\n",
    "print('Last column:', tensor[..., -1])\n",
    "tensor[:,1] = 2\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 1, 2, 3, 1, 2, 3],\n",
      "        [4, 2, 6, 4, 2, 6, 4, 2, 6],\n",
      "        [7, 2, 9, 7, 2, 9, 7, 2, 9]])\n",
      "tensor([[ 3,  6,  9,  3,  6,  9,  3,  6,  9],\n",
      "        [12,  6, 18, 12,  6, 18, 12,  6, 18],\n",
      "        [21,  6, 27, 21,  6, 27, 21,  6, 27]])\n",
      "torch.Size([3, 9])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 2, 6],\n",
      "        [7, 2, 9],\n",
      "        [1, 2, 3],\n",
      "        [4, 2, 6],\n",
      "        [7, 2, 9],\n",
      "        [1, 2, 3],\n",
      "        [4, 2, 6],\n",
      "        [7, 2, 9]])\n",
      "torch.Size([9, 3])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1) #dim=1 表示沿列方向拼接\n",
    "print(t1)\n",
    "print(t1 * 3)\n",
    "print(t1.shape)\n",
    "\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=0) #dim=0 表示沿行方向拼接\n",
    "print(t1)\n",
    "print(t1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 15.,  18.,  21.],\n",
      "        [ 42.,  54.,  66.],\n",
      "        [ 69.,  90., 111.]])\n",
      "\n",
      "tensor([[ 15.,  18.,  21.],\n",
      "        [ 42.,  54.,  66.],\n",
      "        [ 69.,  90., 111.]])\n",
      "\n",
      "tensor([[ 15.,  18.,  21.],\n",
      "        [ 42.,  54.,  66.],\n",
      "        [ 69.,  90., 111.]])\n",
      "\n",
      "tensor([[ 0.,  1.,  4.],\n",
      "        [ 9., 16., 25.],\n",
      "        [36., 49., 64.]])\n",
      "\n",
      "tensor([[ 0.,  1.,  4.],\n",
      "        [ 9., 16., 25.],\n",
      "        [36., 49., 64.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "tensor = torch.arange(9, dtype=torch.float32).reshape(3, 3)\n",
    "\n",
    "# 计算两个张量之间矩阵乘法的几种方式。 y1, y2, y3 最后的值是一样的 dot\n",
    "y1 = tensor @ tensor\n",
    "y2 = tensor.matmul(tensor)\n",
    "\n",
    "print(y1)\n",
    "print()\n",
    "print(y2)\n",
    "print()\n",
    "\n",
    "y3 = torch.rand_like(tensor)\n",
    "torch.matmul(tensor, tensor, out=y3)\n",
    "print(y3)\n",
    "print()\n",
    "\n",
    "\n",
    "# 计算张量逐元素相乘的几种方法。 z1, z2, z3 最后的值是一样的。\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)\n",
    "\n",
    "print(z1)\n",
    "print()\n",
    "print(z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "agg = tensor.sum() ## 创建一个 3x3 的张量\n",
    "agg_item = agg.item() ## 将结果转换为 Python 标量\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  4.],\n",
       "       [ 9., 16., 25.],\n",
       "       [36., 49., 64.]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_arr = z1.numpy() #将 tensor 转化为 numpy\n",
    "np_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.,  6.,  7.],\n",
      "        [ 8.,  9., 10.],\n",
      "        [11., 12., 13.]]) \n",
      "\n",
      "tensor([[ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.],\n",
      "        [12., 13., 14.]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor, \"\\n\")\n",
    "tensor.add_(1)\n",
    "# tensor = tensor + 5\n",
    "# tensor += 5\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.,  7.,  8.],\n",
       "        [ 9., 10., 11.],\n",
       "        [12., 13., 14.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TianXiang\\AppData\\Local\\Temp\\ipykernel_13976\\4011233948.py:11: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3701.)\n",
      "  result = torch.matmul(a, x.T) + torch.mul(b, x) + c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'expression.png'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "\n",
    "# 定义矩阵 a，向量 b 和常数 c\n",
    "a = torch.randn(10, 10, requires_grad=True)  # requires_grad=True 表示我们要对 A 求导/梯度\n",
    "b = torch.randn(10, 10, requires_grad=True)\n",
    "c = torch.randn(1, requires_grad=True)\n",
    "x = torch.randn(10, requires_grad=True)\n",
    "\n",
    "# 计算 x^T * A + b * x + c\n",
    "result = torch.matmul(a, x.T) + torch.mul(b, x) + c\n",
    "\n",
    "# # 生成计算图节点\n",
    "dot = make_dot(result, params={'a': a, 'b': b, 'c': c, 'x': x})\n",
    "# # 绘制计算图\n",
    "dot.render('expression', format='png', cleanup=True, view=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
