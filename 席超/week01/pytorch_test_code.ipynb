{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]], dtype=torch.int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#张量\n",
    "\n",
    "#从Numpy数组中创建\n",
    "a = np.array([(1,2,3),(4,5,6)])\n",
    "x_np = torch.from_numpy(a)\n",
    "x_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10, 20, 30],\n",
      "        [40, 50, 60]])\n"
     ]
    }
   ],
   "source": [
    "#直接从数据中创建\n",
    "data = [[10,20,30],[40,50,60]]\n",
    "# data = [[10,20,30]]\n",
    "x_data = torch.tensor(data)\n",
    "print(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor:\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int32)\n",
      " torch.int32\n"
     ]
    }
   ],
   "source": [
    "#从另一张量中创建\n",
    "x_ones= torch.ones_like(x_np)\n",
    "print(f\"Ones Tensor:\\n{x_ones}\\n\",x_ones.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0181, 0.0667, 0.5319],\n",
       "        [0.9455, 0.1500, 0.0742]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#从另一张量中创建\n",
    "x_rand = torch.rand_like(x_data,dtype=torch.float)      #覆盖原有x_data的数据类型\n",
    "x_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_ones.shape: torch.Size([2, 3])\n",
      "x_data.shape: torch.Size([2, 3])\n",
      "x_rand.shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"x_ones.shape:\",x_ones.shape)\n",
    "print(\"x_data.shape:\",x_data.shape)\n",
    "print(\"x_rand.shape:\",x_rand.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor:\n",
      "tensor([[0.8533, 0.1975, 0.8439],\n",
      "        [0.6573, 0.0848, 0.9115]])\n",
      "\n",
      "Ones Tensor\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "Zeros Tensor\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使⽤随机值或常量值：\n",
    "#shape 是张量维度的元组。在下⾯的函数中，它决定了输出张量的维度。\n",
    "shape = (2,3)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "print(f\"Random Tensor:\\n{rand_tensor}\\n\")\n",
    "print(f\"Ones Tensor\\n{ones_tensor}\\n\")\n",
    "print(f\"Zeros Tensor\\n{zeros_tensor}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5596, 0.7206, 0.2840],\n",
       "        [0.7208, 0.2587, 0.1193],\n",
       "        [0.6035, 0.1569, 0.5012],\n",
       "        [0.5108, 0.9487, 0.3274],\n",
       "        [0.7491, 0.1271, 0.8043]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#其他一些创建方法\n",
    "#基于现有tensor构建，但使用新值填充\n",
    "m = torch.ones(5,3,dtype=torch.double)\n",
    "n = torch.rand_like(m,dtype=torch.float)\n",
    "\n",
    "#获取tensor的大小\n",
    "print(m.size())\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0724, 0.5631, 0.6668],\n",
       "        [0.4579, 0.4147, 0.8994],\n",
       "        [0.7902, 0.8423, 0.9427],\n",
       "        [0.2016, 0.2564, 0.8735],\n",
       "        [0.6420, 0.3355, 0.8014]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#均匀分布\n",
    "torch.rand(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.4674, -0.9658, -0.5749],\n",
       "        [-0.8805,  0.2326,  3.1471],\n",
       "        [-0.1655, -1.3269,  0.5015],\n",
       "        [-2.0211,  1.3892, -0.8160],\n",
       "        [ 0.2059, -0.4951,  1.3306]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#标准正态分布\n",
    "torch.randn(5,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5155,  1.2276,  2.2863],\n",
       "        [-0.7181, -1.1068,  0.2950],\n",
       "        [ 0.0577, -0.4889,  0.1441],\n",
       "        [ 0.5195, -0.4705, -0.0938],\n",
       "        [ 0.5296,  0.3952, -0.9055]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#离散正态分布\n",
    "torch.normal(mean=.0,std=1.0,size=(5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5000,  1.0000,  1.5000,  2.0000,  2.5000,  3.0000,  3.5000,  4.0000,\n",
       "         4.5000,  5.0000,  5.5000,  6.0000,  6.5000,  7.0000,  7.5000,  8.0000,\n",
       "         8.5000,  9.0000,  9.5000, 10.0000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 线性间隔向量(返回⼀个1维张量，包含在区间start和end上均匀间隔的steps个点)\n",
    "torch.linspace(start=0.5,end=10,steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shaoe of tensor:\ttorch.Size([5, 6])\n",
      "Datatype of tensor:\ttorch.float32\n",
      "Device of tensor:\tcpu\n",
      "tensor([[0.8761, 0.5700, 0.7347, 0.9524, 0.8829, 0.0802],\n",
      "        [0.6776, 0.2437, 0.8173, 0.7736, 0.3970, 0.7755],\n",
      "        [0.6735, 0.7870, 0.0069, 0.8403, 0.1179, 0.7385],\n",
      "        [0.2520, 0.9926, 0.0391, 0.1035, 0.7234, 0.7513],\n",
      "        [0.0259, 0.4594, 0.7978, 0.2533, 0.7868, 0.4389]])\n"
     ]
    }
   ],
   "source": [
    "#张量的属性\n",
    "tensor = torch.rand(5,6)\n",
    "print(f\"Shaoe of tensor:\\t{tensor.shape}\")\n",
    "print(f\"Datatype of tensor:\\t{tensor.dtype}\")\n",
    "print(f\"Device of tensor:\\t{tensor.device}\")\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#张量的运算\n",
    "#默认情况张量在CPU上运算\n",
    "#判断是否支持在GPU上运算\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5370, -0.9013, -1.6874, -1.2567, -0.3504],\n",
      "        [-2.1047,  0.3546,  0.3100, -1.1826, -0.0795],\n",
      "        [-1.9480, -0.9064, -0.5035, -0.9696,  0.0187],\n",
      "        [ 1.9089, -1.7725,  0.7393, -0.8591, -1.7596],\n",
      "        [ 0.5382,  1.9071,  0.7776, -0.3306,  0.5056]])\n",
      "cpu\n",
      "tensor([[-0.5370, -0.9013, -1.6874, -1.2567, -0.3504],\n",
      "        [-2.1047,  0.3546,  0.3100, -1.1826, -0.0795],\n",
      "        [-1.9480, -0.9064, -0.5035, -0.9696,  0.0187],\n",
      "        [ 1.9089, -1.7725,  0.7393, -0.8591, -1.7596],\n",
      "        [ 0.5382,  1.9071,  0.7776, -0.3306,  0.5056]])\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.randn(5,5)\n",
    "# 检查pytorch是否支持GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    tensor = tensor.to(device)\n",
    "\n",
    "print(tensor)\n",
    "print(tensor.device)\n",
    "\n",
    "# mac上没有GPU，使用M系列芯片\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    tensor = tensor.to(device)\n",
    "\n",
    "print(tensor)\n",
    "print(tensor.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row:\ttensor([1, 2, 3, 4], dtype=torch.int32)\n",
      "First Column:\ttensor([ 1,  5,  9, 13], dtype=torch.int32)\n",
      "Last Column:\ttensor([ 4,  8, 12, 16], dtype=torch.int32)\n",
      "Last Column:\ttensor([ 4,  8, 12, 16], dtype=torch.int32)\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12],\n",
      "        [13, 14, 15, 16]], dtype=torch.int32)\n",
      "tensor([[ 0,  0,  0,  0],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12],\n",
      "        [13, 14, 15, 16]], dtype=torch.int32)\n",
      "tensor([[ 0,  2,  0,  0],\n",
      "        [ 5,  2,  7,  8],\n",
      "        [ 9,  2, 11, 12],\n",
      "        [13,  2, 15, 16]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "#张量的索引和切片\n",
    "nd = np.array([(1,2,3,4),(5,6,7,8),(9,10,11,12),(13,14,15,16)])\n",
    "tensor = torch.from_numpy(nd)\n",
    "print(f\"First row:\\t{tensor[0]}\")\n",
    "print(f\"First Column:\\t{tensor[:,0]}\")\n",
    "print(f\"Last Column:\\t{tensor[:,-1]}\")\n",
    "print(f\"Last Column:\\t{tensor[...,-1]}\")    #通用写法\n",
    "print(tensor)\n",
    "tensor[:1] = 0\n",
    "print(tensor)\n",
    "tensor[:,1] = 2\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1,   2,   3,  10,  20,  30,   1,   2,   3,  10,  20,  30],\n",
       "        [  4,   5,   6,  40,  50,  60,   4,   5,   6,  40,  50,  60],\n",
       "        [  7,   8,   9,  70,  80,  90,   7,   8,   9,  70,  80,  90],\n",
       "        [ 10,  11,  12, 100, 110, 120,  10,  11,  12, 100, 110, 120]],\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#张量的拼接\n",
    "t1 = torch.from_numpy(np.array([(1,2,3),(4,5,6),(7,8,9),(10,11,12)]))\n",
    "t2 = torch.from_numpy(np.array([(10,20,30),(40,50,60),(70,80,90),(100,110,120)]))\n",
    "t3 = torch.cat([t1,t2,t1,t2],dim = 1)\n",
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1,   2,   3],\n",
       "        [  4,   5,   6],\n",
       "        [  7,   8,   9],\n",
       "        [ 10,  11,  12],\n",
       "        [ 10,  20,  30],\n",
       "        [ 40,  50,  60],\n",
       "        [ 70,  80,  90],\n",
       "        [100, 110, 120],\n",
       "        [  1,   2,   3],\n",
       "        [  4,   5,   6],\n",
       "        [  7,   8,   9],\n",
       "        [ 10,  11,  12],\n",
       "        [ 10,  20,  30],\n",
       "        [ 40,  50,  60],\n",
       "        [ 70,  80,  90],\n",
       "        [100, 110, 120]], dtype=torch.int32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.from_numpy(np.array([(1,2,3),(4,5,6),(7,8,9),(10,11,12)]))\n",
    "t2 = torch.from_numpy(np.array([(10,20,30),(40,50,60),(70,80,90),(100,110,120)]))\n",
    "t3 = torch.cat([t1,t2,t1,t2],dim = 0)\n",
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n",
      "tensor([[5, 6],\n",
      "        [7, 8]], dtype=torch.int32)\n",
      "\n",
      "tensor([[5, 7],\n",
      "        [6, 8]], dtype=torch.int32)\n",
      "\n",
      "tensor([[17, 23],\n",
      "        [39, 53]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "#算数运算\n",
    "#计算两个张量之间矩阵乘法\n",
    "y1 = torch.from_numpy(np.array([(1,2),(3,4)]))\n",
    "y2 = torch.from_numpy(np.array([(5,6),(7,8)]))\n",
    "print(y1)\n",
    "print(y2)\n",
    "print()\n",
    "print(y2.T)\n",
    "print()\n",
    "result1 = y1 @ y2.T     #.的优先级高于@\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.],\n",
      "        [13., 14., 15., 16.]])\n",
      "\n",
      "tensor([[ 1.,  5.,  9., 13.],\n",
      "        [ 2.,  6., 10., 14.],\n",
      "        [ 3.,  7., 11., 15.],\n",
      "        [ 4.,  8., 12., 16.]])\n",
      "\n",
      "tensor * tensor:\n",
      "tensor([[  1.,   4.,   9.,  16.],\n",
      "        [ 25.,  36.,  49.,  64.],\n",
      "        [ 81., 100., 121., 144.],\n",
      "        [169., 196., 225., 256.]])\n",
      "tensor.mul(tensor):\n",
      "tensor([[  1.,   4.,   9.,  16.],\n",
      "        [ 25.,  36.,  49.,  64.],\n",
      "        [ 81., 100., 121., 144.],\n",
      "        [169., 196., 225., 256.]])\n",
      "torch.mul(tensor,tensor,out=z3):\n",
      "tensor([[  1.,   4.,   9.,  16.],\n",
      "        [ 25.,  36.,  49.,  64.],\n",
      "        [ 81., 100., 121., 144.],\n",
      "        [169., 196., 225., 256.]])\n",
      "torch.mul(tensor,3,out=z4):\n",
      "tensor([[ 4.,  8., 12., 16.],\n",
      "        [20., 24., 28., 32.],\n",
      "        [36., 40., 44., 48.],\n",
      "        [52., 56., 60., 64.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.arange(1,17,dtype=torch.float32).reshape(4,4)\n",
    "print(tensor)\n",
    "print()\n",
    "print(tensor.T)\n",
    "print()\n",
    "# #计算两个张量之间矩阵乘法的几种方式，y1、y2、y3最后的值是一样的dot\n",
    "# y1 = tensor @ tensor.T\n",
    "# y2 = tensor.matmul(tensor.T)      #matmul和numpy中的dot是一样的\n",
    "# y3 = torch.rand_like(tensor)\n",
    "# torch.matmul(tensor,tensor.T,out=y3)    #out=yy3覆盖y3原先的值\n",
    "\n",
    "# print(\"tensor @ tensor.T:\")\n",
    "# print(y1)\n",
    "# print(\"tensor.matmul(tensor.T):\")\n",
    "# print(y2) \n",
    "# print(\"torch.matmul(tensor,tensor.T,out=y3):\")\n",
    "# print(y3)\n",
    "\n",
    "#计算张量元素相乘的几种方法。z1、z2、z3最后的值是一样的。\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor,tensor,out=z3)     #out=z3覆盖z3原先的值\n",
    "\n",
    "#每一位乘以4\n",
    "z4 = torch.rand_like(tensor)\n",
    "torch.mul(tensor,4,out=z4)\n",
    "\n",
    "print(\"tensor * tensor:\")\n",
    "print(z1)\n",
    "print(\"tensor.mul(tensor):\")\n",
    "print(z2)\n",
    "print(\"torch.mul(tensor,tensor,out=z3):\")\n",
    "print(z3)\n",
    "print(\"torch.mul(tensor,3,out=z4):\")\n",
    "print(z4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.],\n",
      "        [13., 14., 15., 16.]])\n",
      "tensor(136.)\n",
      "136.0 <class 'float'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.,  3.,  4.],\n",
       "       [ 5.,  6.,  7.,  8.],\n",
       "       [ 9., 10., 11., 12.],\n",
       "       [13., 14., 15., 16.]], dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#单元素张量\n",
    "#如果一个单元素张量，例如张量的值聚合计算，可以使用item()方法将其转换为Python数值\n",
    "print(tensor)\n",
    "agg = tensor.sum()\n",
    "agg_item = agg.item()   #返回python的中原生的基本类型\n",
    "print(agg)\n",
    "print(agg_item,type(agg_item))\n",
    "\n",
    "#对于更高纬的数据不能用Python的类型来接收，可以将它转换为Numpy的ndarray数组再来进行本地化的操作或运算\n",
    "np_arr = tensor.numpy()\n",
    "np_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.],\n",
      "        [13., 14., 15., 16.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In-place操作      谨慎使用！！！\n",
    "#把计结果存储到当前操作数中的操作就称为就地操作。含义和pandas中的inPlace参数的含义一样。pytorch中，这些操作是由带有下划线_后缀的函数表示。\n",
    "#例如：x.copy_(y),x.t_(),将改变x自身的值。\n",
    "print(tensor,\"\\n\")\n",
    "tensor.add_(10)     #没有返回的结果值，将计算的结果直接更新到张量，张量内部的值直接就被更新了\n",
    "# 和tensor = tensor + 5或tensor += 5效果相同，In-place操作会节省一部分内存空间，效率更高，但在计算导数时可能会出现问题，因为他会立即丢失历史记录。因此，不鼓励使用他们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#与numpy之间的转换\n",
    "#张量到numpy数组\n",
    "t = torch.ones(5,5)\n",
    "print(t)\n",
    "n = t.numpy()\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10., 10., 10., 10., 10.],\n",
      "        [10., 10., 10., 10., 10.],\n",
      "        [10., 10., 10., 10., 10.],\n",
      "        [10., 10., 10., 10., 10.],\n",
      "        [10., 10., 10., 10., 10.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[10., 10., 10., 10., 10.],\n",
       "       [10., 10., 10., 10., 10.],\n",
       "       [10., 10., 10., 10., 10.],\n",
       "       [10., 10., 10., 10., 10.],\n",
       "       [10., 10., 10., 10., 10.]], dtype=float32)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#张量值的变更也反应在关联的Numpy数组中\n",
    "# t.add_(3)\n",
    "print(t)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#numpy数组到张量\n",
    "n1 = np.eye(4)\n",
    "t1 = torch.from_numpy(n1)\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1:tensor([[2., 1., 1., 1.],\n",
      "        [1., 2., 1., 1.],\n",
      "        [1., 1., 2., 1.],\n",
      "        [1., 1., 1., 2.]], dtype=torch.float64)\n",
      "n1:[[2. 1. 1. 1.]\n",
      " [1. 2. 1. 1.]\n",
      " [1. 1. 2. 1.]\n",
      " [1. 1. 1. 2.]]\n"
     ]
    }
   ],
   "source": [
    "#numpy数组的变化也反应在张量中\n",
    "np.add(n1,1,out=n1)\n",
    "print(f\"t1:{t1}\")\n",
    "print(f\"n1:{n1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'expression.png'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#计算图\n",
    "import torch\n",
    "from torchviz import make_dot\n",
    "\n",
    "#定义矩阵A,向量b和常数c\n",
    "A = torch.randn(5,5,requires_grad=True)     #requires_grad=True表示对A求导\n",
    "b = torch.randn(5,requires_grad=True)\n",
    "c = torch.randn(1,requires_grad=True)\n",
    "x = torch.randn(5,requires_grad=True)\n",
    "\n",
    "#计算x^T * A + b * x +c\n",
    "result = torch.matmul(A, x.T) + torch.matmul(b,x) + c\n",
    "#生成计算图节点\n",
    "dot = make_dot(result,params={\"A\":A,\"b\":b,\"c\":c,\"x\":x})\n",
    "#绘制计算图\n",
    "dot.render(\"expression\",format=\"png\",cleanup=True,view=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
