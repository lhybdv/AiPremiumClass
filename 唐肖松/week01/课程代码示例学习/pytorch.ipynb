{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 创建一个张量\n",
    "data = torch.tensor([[1,2],[3,4],[5,6]], dtype=torch.float)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "2.6.0+cpu\n",
      "2.2.0\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "# 通过numpy创建一个张量\n",
    "np_array = np.array([[1,2],[3,4],[5,6]])\n",
    "print(np_array)\n",
    "print(torch.__version__)\n",
    "print(np.__version__)\n",
    "data = torch.from_numpy(np_array)\n",
    "print(data)\n",
    "# 数据类型\n",
    "print(data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7759, 0.0649],\n",
       "        [0.0947, 0.1582],\n",
       "        [0.3745, 0.1646]], dtype=torch.float16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.rand_like 通过已知张量来创建新的张量 注意指定张量内部数据类型\n",
    "data2 = torch.rand_like(data, dtype=torch.float16)\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.9667, 0.0901, 0.1856],\n",
      "        [0.8368, 0.2179, 0.1342],\n",
      "        [0.7433, 0.8042, 0.6795]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (3,3) # shape 张量维度\n",
    "rand_tensor = torch.rand(shape) # 指定维度下随机一个张量\n",
    "ones_tensor = torch.ones(shape) # 指定维度下初始化一个全部为1的张量\n",
    "zeros_tensor = torch.zeros(shape) # 指定维度下初始化一个全部为0的张量\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5])\n",
      "tensor([[0.8712, 0.1311, 0.1182, 0.1941, 0.9303],\n",
      "        [0.6861, 0.4497, 0.7107, 0.2813, 0.2676],\n",
      "        [0.3276, 0.3615, 0.6664, 0.7961, 0.4990]])\n",
      "tensor([[-0.7588, -1.3232,  0.6431,  0.0964,  0.8541],\n",
      "        [ 0.0840, -1.1206, -0.2369,  1.2832,  0.0087],\n",
      "        [-0.9147,  0.8657, -0.3849, -1.5592,  0.2866]])\n",
      "tensor([[ 0.7207, -0.4677, -0.9454,  0.7172,  1.1158],\n",
      "        [ 0.8505,  0.5364, -0.1640, -0.7723, -0.4060],\n",
      "        [ 0.2089,  0.3053, -0.3013,  1.3968,  1.2452]])\n",
      "tensor([ 1.0000,  1.4737,  1.9474,  2.4211,  2.8947,  3.3684,  3.8421,  4.3158,\n",
      "         4.7895,  5.2632,  5.7368,  6.2105,  6.6842,  7.1579,  7.6316,  8.1053,\n",
      "         8.5789,  9.0526,  9.5263, 10.0000])\n"
     ]
    }
   ],
   "source": [
    "# 基于现有tensor构建，但使⽤新值填充\n",
    "m = torch.ones(3,5, dtype=torch.double)\n",
    "n = torch.rand_like(m, dtype=torch.float)\n",
    "# 获取tensor的⼤⼩\n",
    "print(m.size()) # torch.Size([3,5])\n",
    "# 均匀分布\n",
    "print(torch.rand(3,5))\n",
    "# 标准正态分布\n",
    "print(torch.randn(3,5))\n",
    "# 离散正态分布\n",
    "print(torch.normal(mean=.0,std=1.0,size=(3,5)))\n",
    "# 线性间隔向量(返回⼀个1维张量，包含在区间start和end上均匀间隔的steps个点)\n",
    "print(torch.linspace(start=1,end=10,steps=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\") # shape 张量的维度大小\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\") # dtype 张量的数据类型\n",
    "print(f\"Device tensor is stored on: {tensor.device}\") # 张量在哪个设备上存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7064, 0.7501, 0.3071, 0.7946],\n",
      "        [0.5073, 0.4343, 0.2472, 0.0408],\n",
      "        [0.8393, 0.1950, 0.8128, 0.4901]])\n",
      "cpu\n",
      "tensor([[0.7064, 0.7501, 0.3071, 0.7946],\n",
      "        [0.5073, 0.4343, 0.2472, 0.0408],\n",
      "        [0.8393, 0.1950, 0.8128, 0.4901]])\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# 检查pytorch是否支持GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    tensor = tensor.to(device)\n",
    "\n",
    "print(tensor)\n",
    "print(tensor.device)\n",
    "\n",
    "# mac上没有GPU，使用M系列芯片\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    tensor = tensor.to(device)\n",
    "\n",
    "print(tensor)\n",
    "print(tensor.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row:  tensor([0., 0., 0.])\n",
      "First column:  tensor([0., 0., 0., 0.])\n",
      "Last column: tensor([0., 0., 0., 0.])\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.zeros(4, 3)\n",
    "print('First row: ', tensor[0]) \n",
    "print('First column: ', tensor[:, 0])\n",
    "print('Last column:', tensor[..., -1]) # -1表示倒数第一个  ... 用来表示前置所有\n",
    "tensor[:,1] = 1\n",
    "print(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n",
      "tensor([[0., 3., 0.],\n",
      "        [0., 3., 0.],\n",
      "        [0., 3., 0.],\n",
      "        [0., 3., 0.],\n",
      "        [0., 3., 0.],\n",
      "        [0., 3., 0.],\n",
      "        [0., 3., 0.],\n",
      "        [0., 3., 0.],\n",
      "        [0., 3., 0.],\n",
      "        [0., 3., 0.],\n",
      "        [0., 3., 0.],\n",
      "        [0., 3., 0.]])\n",
      "torch.Size([12, 3])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=-2) #cat 拼接张量 dim指定维度\n",
    "print(t1)\n",
    "print(t1 * 3)\n",
    "print(t1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n",
      "tensor([[ 14.,  32.,  50.],\n",
      "        [ 32.,  77., 122.],\n",
      "        [ 50., 122., 194.]])\n",
      "tensor([[ 14.,  32.,  50.],\n",
      "        [ 32.,  77., 122.],\n",
      "        [ 50., 122., 194.]])\n",
      "tensor([[ 14.,  32.,  50.],\n",
      "        [ 32.,  77., 122.],\n",
      "        [ 50., 122., 194.]])\n",
      "tensor([[ 1.,  4.,  9.],\n",
      "        [16., 25., 36.],\n",
      "        [49., 64., 81.]])\n",
      "tensor([[ 1.,  4.,  9.],\n",
      "        [16., 25., 36.],\n",
      "        [49., 64., 81.]])\n",
      "tensor([[ 1.,  4.,  9.],\n",
      "        [16., 25., 36.],\n",
      "        [49., 64., 81.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.arange(1,10, dtype=torch.float32).reshape(3, 3)\n",
    "print(tensor)\n",
    "# 计算两个张量之间矩阵乘法的几种方式。 y1, y2, y3 最后的值是一样的 dot\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "print(y1)\n",
    "print(y2)\n",
    "\n",
    "y3 = torch.rand_like(tensor)\n",
    "torch.matmul(tensor, tensor.T, out=y3) # out指定输出到y3\n",
    "print(y3)\n",
    "\n",
    "\n",
    "# 计算张量逐元素相乘的几种方法。 z1, z2, z3 最后的值是一样的。\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)\n",
    "\n",
    "print(z1)\n",
    "print(z2)\n",
    "print(z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(45.)\n",
      "45.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "agg = tensor.sum() # sum 将张量中的所有元素相加\n",
    "print(agg)\n",
    "agg_item = agg.item() # item 将一个单元素的张量转化为python数值\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.],\n",
       "       [49., 64., 81.]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_arr = z1.numpy() # numpy() 将张量转化为numpy数组\n",
    "np_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]]) \n",
      "\n",
      "tensor([[ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.],\n",
      "        [12., 13., 14.]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5) # 等同于如下两个方法 会改变tensor原张量的值，称之为in-place操作\n",
    "# tensor = tensor + 5\n",
    "# tensor += 5\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\83668\\AppData\\Local\\Temp\\ipykernel_19700\\125038839.py:11: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3729.)\n",
      "  result = torch.matmul(A, x.T) + torch.matmul(b, x) + c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.8969,  3.4667,  0.9040,  6.8076, -0.7734, -0.2501,  4.1617, -3.3890,\n",
      "        -4.1655, -1.7984], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'expression.png'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "# 定义矩阵 A，向量 b 和常数 c\n",
    "A = torch.randn(10, 10,requires_grad=True)  # requires_grad=True 表示我们要对 A 求导\n",
    "b = torch.randn(10,requires_grad=True)\n",
    "c = torch.randn(1,requires_grad=True)\n",
    "x = torch.randn(10, requires_grad=True)\n",
    "\n",
    "\n",
    "# 计算 x^T * A + b * x + c\n",
    "result = torch.matmul(A, x.T) + torch.matmul(b, x) + c\n",
    "print(result)\n",
    "# 生成计算图节点\n",
    "dot = make_dot(result, params={'A': A, 'b': b, 'c': c, 'x': x})\n",
    "# 绘制计算图\n",
    "dot.render('expression', format='png', cleanup=True, view=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
