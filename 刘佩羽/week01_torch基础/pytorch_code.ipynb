{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch基础-张量\n",
    "\n",
    "张量（Tensor）\n",
    "\n",
    "张量（Tensor）是pytorch中的基本单位，也是深度学习框架构成的重要组成。\n",
    "\n",
    "我们可以先把张量看做是⼀个容器，⾥⾯承载了需要运算的数据。\n",
    "\n",
    "张量可以通过多种⽅式初始化。看看下⾯的例⼦：\n",
    "直接从数据\n",
    "\n",
    "张量可以直接从数据中创建。数据类型是⾃动推断的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [2, 3], [3, 4]]\n",
      "tensor([[1, 2],\n",
      "        [2, 3],\n",
      "        [3, 4]])\n",
      "torch.float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# 创建一个二维列表\n",
    "data = [[1,2],[2,3],[3,4]]\n",
    "# 将二维列表转换为张量\n",
    "data_tensor = torch.tensor(data=data)\n",
    "# 打印二维列表\n",
    "print(data)\n",
    "# 打印张量\n",
    "print(data_tensor)\n",
    "# 创建一个二维列表，并指定数据类型为float32\n",
    "data = torch.tensor([[1,2],[3,4]], dtype=torch.float32)\n",
    "# 打印数据类型\n",
    "print(data.dtype)\n",
    "# 打印张量\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor（张量）和numpy_array(数组)转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32) <class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "# 创建一个numpy数组\n",
    "np_array = np.array([[1,2],[3,4]])\n",
    "print(type(np_array))\n",
    "# 将numpy数组转换为tensor\n",
    "data_tensor = torch.from_numpy(np_array)\n",
    "print(data_tensor,type(data_tensor))\n",
    "# 将tensor转换为numpy数组\n",
    "np_array = data_tensor.numpy()\n",
    "np_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor的属性和方法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int32\n",
      "torch.Size([2, 2])\n",
      "2\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# 打印data_tensor的数据类型\n",
    "print(data_tensor.dtype)\n",
    "# 打印data_tensor的形状\n",
    "print(data_tensor.shape)\n",
    "# 打印data_tensor的维度\n",
    "print(data_tensor.ndim)\n",
    "# 打印data_tensor所在的设备\n",
    "print(data_tensor.device)\n",
    "# print(data_tensor.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_tensor.shaped:  torch.Size([3, 2])\n",
      "data3.shape:  torch.Size([3, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.8982, 0.2162],\n",
       "        [0.4869, 0.5964],\n",
       "        [0.8275, 0.0211]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 打印data_tensor的形状\n",
    "print('data_tensor.shape: ',data_tensor.shape)\n",
    "# 创建一个与data_tensor形状相同，数据类型为float的随机张量-->torch.tensor_like(data_tensor,dtype=float) x 应该为torch.rand_like(data_tensor,dtype=float)\n",
    "data3 = torch.rand_like(data_tensor, dtype=torch.float)\n",
    "# 打印data3的形状\n",
    "print('data3.shape: ',data3.shape)\n",
    "# 返回data3\n",
    "data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.1617, 0.4225, 0.0395],\n",
      "        [0.1011, 0.6725, 0.4511]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 定义一个形状为(2,3,)的张量\n",
    "shape = (2,3,) #  定义一个形状为(2,3)的元组\n",
    "rand_tensor = torch.rand(shape) #  生成一个指定形状的随机张量 torch.tensor(shape:tuple)\n",
    "ones_tensor = torch.ones(shape) #  生成一个形状为(2,3)的全1张量 torch.ones((2,3))\n",
    "zeros_tensor = torch.zeros(shape) #  生成一个形状为(2,3)的全0张量torch.zeros((2,3))\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\") #  打印随机张量\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\") #  打印全1张量\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\") #  打印全0张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3]) \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "======================\n",
      "tensor([[0.5648, 0.7737, 0.0239],\n",
      "        [0.8702, 0.9839, 0.3671],\n",
      "        [0.8697, 0.3623, 0.4183],\n",
      "        [0.4954, 0.4076, 0.5233],\n",
      "        [0.4497, 0.6551, 0.2449]])\n",
      "tensor([[ 1.7889, -0.7664, -1.4013],\n",
      "        [-0.5463, -0.1856, -0.2040],\n",
      "        [ 1.0264, -1.7208,  0.6240],\n",
      "        [-1.0375, -0.5275, -1.1950],\n",
      "        [ 1.0389,  0.7380,  0.1551]])\n",
      "tensor([[ 0.8706,  0.2942,  1.9502],\n",
      "        [ 0.2671, -0.1697, -0.7264],\n",
      "        [ 0.2316,  1.6486, -0.5303],\n",
      "        [-0.8716, -0.0133, -0.2651],\n",
      "        [-1.9328, -1.3316, -0.1177]])\n",
      "tensor([ 1.0000,  1.4500,  1.9000,  2.3500,  2.8000,  3.2500,  3.7000,  4.1500,\n",
      "         4.6000,  5.0500,  5.5000,  5.9500,  6.4000,  6.8500,  7.3000,  7.7500,\n",
      "         8.2000,  8.6500,  9.1000,  9.5500, 10.0000])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个5行3列的全1矩阵，数据类型为双精度浮点数\n",
    "m = torch.ones(5,3, dtype=torch.double)\n",
    "# 创建一个与m形状相同的随机矩阵，数据类型为单精度浮点数\n",
    "n = torch.rand_like(m, dtype=torch.float)\n",
    "# 打印m的形状\n",
    "print(m.size(),'\\n',m) # torch.Size([5,3])\n",
    "\n",
    "print('======================')\n",
    "\n",
    "# 打印一个5行3列的随机矩阵\n",
    "print(torch.rand(5,3))\n",
    "# 打印一个5行3列的正态分布随机矩阵\n",
    "print(torch.randn(5,3))\n",
    "# 打印一个5行3列的正态分布随机矩阵，均值为0，标准差为1\n",
    "print(torch.normal(mean=.0,std=1.0,size=(5,3)))\n",
    "# 打印一个从1到10的等间隔的21个数的张量\n",
    "print(torch.linspace(start=1,end=10,steps=21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "# 创建一个3行4列的随机张量\n",
    "tensor = torch.rand(3,4)\n",
    "\n",
    "# 打印张量的形状\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "# 打印张量的数据类型\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "# 打印张量存储的设备\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4028, 0.9439, 0.1505, 0.3773],\n",
      "        [0.0966, 0.0682, 0.3893, 0.9114],\n",
      "        [0.9544, 0.5207, 0.9319, 0.7016]])\n",
      "cpu\n",
      "tensor([[0.4028, 0.9439, 0.1505, 0.3773],\n",
      "        [0.0966, 0.0682, 0.3893, 0.9114],\n",
      "        [0.9544, 0.5207, 0.9319, 0.7016]])\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# 检查pytorch是否支持GPU\n",
    "if torch.cuda.is_available():\n",
    "    # 如果支持GPU，则将设备设置为GPU\n",
    "    device = torch.device(\"cuda\")\n",
    "    tensor = tensor.to(device)\n",
    "\n",
    "print(tensor)\n",
    "print(tensor.device)\n",
    "\n",
    "# mac上没有GPU，使用M系列芯片\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    tensor = tensor.to(device)\n",
    "\n",
    "print(tensor)\n",
    "print(tensor.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row:  tensor([1., 1., 1., 1.])\n",
      "First column:  tensor([1., 1., 1., 1.])\n",
      "Last column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个4x4的全1张量\n",
    "tensor = torch.ones(4, 4)\n",
    "# 打印第一行\n",
    "print('First row: ', tensor[0])\n",
    "# 打印第一列\n",
    "print('First column: ', tensor[:, 0])\n",
    "# 打印最后一列\n",
    "print('Last column:', tensor[..., -1])\n",
    "# 将第二列的值全部设为0\n",
    "tensor[:,1] = 0\n",
    "# 打印修改后的张量\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "torch.Size([12, 4])\n",
      "tensor([[3., 0., 3., 3., 3., 0., 3., 3., 3., 0., 3., 3.],\n",
      "        [3., 0., 3., 3., 3., 0., 3., 3., 3., 0., 3., 3.],\n",
      "        [3., 0., 3., 3., 3., 0., 3., 3., 3., 0., 3., 3.],\n",
      "        [3., 0., 3., 3., 3., 0., 3., 3., 3., 0., 3., 3.]])\n",
      "torch.Size([4, 12])\n"
     ]
    }
   ],
   "source": [
    "# 将tensor与自身拼接三次，沿着第0维拼接\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=0)\n",
    "print(t1)  # 打印拼接后的tensor\n",
    "print(t1.shape)  # 打印拼接后的tensor的形状\n",
    "# 将tensor与自身拼接三次，沿着第1维拼接\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1 * 3)  # 将拼接后的tensor乘以3\n",
    "print(t1.shape)  # 打印拼接后的tensor的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 14.,  32.,  50.],\n",
      "        [ 32.,  77., 122.],\n",
      "        [ 50., 122., 194.]])\n",
      "tensor([[ 14.,  32.,  50.],\n",
      "        [ 32.,  77., 122.],\n",
      "        [ 50., 122., 194.]])\n",
      "y3:\n",
      " tensor([[0.1259, 0.5341, 0.5635],\n",
      "        [0.1361, 0.6661, 0.6179],\n",
      "        [0.2140, 0.2239, 0.3957]])\n",
      "torch.matmul(tensor, tensor.T, out=y3)   y3:\n",
      " tensor([[ 14.,  32.,  50.],\n",
      "        [ 32.,  77., 122.],\n",
      "        [ 50., 122., 194.]])\n",
      "tensor * tensor: tensor([[ 1.,  4.,  9.],\n",
      "        [16., 25., 36.],\n",
      "        [49., 64., 81.]])\n",
      "torch.mul(tensor, tensor, out=z3) z3: tensor([[ 1.,  4.,  9.],\n",
      "        [16., 25., 36.],\n",
      "        [49., 64., 81.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "tensor = torch.arange(1,10, dtype=torch.float32).reshape(3, 3)\n",
    "\n",
    "# 计算两个张量之间矩阵乘法的几种方式。 y1, y2, y3 最后的值是一样的 dot\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "print(y1)\n",
    "print(y2)\n",
    "\n",
    "y3 = torch.rand_like(tensor)\n",
    "print('y3:\\n',y3)\n",
    "# 计算tensor和tensor的转置的矩阵乘法，并将结果存储在y3中\n",
    "torch.matmul(tensor, tensor.T, out=y3)\n",
    "print('torch.matmul(tensor, tensor.T, out=y3)   y3:\\n',y3)\n",
    "\n",
    "\n",
    "# 计算张量逐元素相乘的几种方法。 z1, z2, z3 最后的值是一样的。\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "print(\"tensor * tensor:\", z1)\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)\n",
    "\n",
    "print(\"tensor * tensor:\", z1)\n",
    "print(\"torch.mul(tensor, tensor, out=z3) z3:\", z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(45.)\n",
      "45.0 <class 'float'>\n",
      "tensor(45.)\n"
     ]
    }
   ],
   "source": [
    "agg = tensor.sum()\n",
    "print(agg)\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))\n",
    "agg = torch.tensor(agg_item)\n",
    "print(agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  4.  9.]\n",
      " [16. 25. 36.]\n",
      " [49. 64. 81.]] <class 'numpy.ndarray'>\n",
      "tensor([[ 1.,  4.,  9.],\n",
      "        [16., 25., 36.],\n",
      "        [49., 64., 81.]])\n"
     ]
    }
   ],
   "source": [
    "np_arr = z1.numpy()\n",
    "print(np_arr,type(np_arr))\n",
    "tensor = torch.from_numpy(np_arr)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-place操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  4.,  9.],\n",
      "        [16., 25., 36.],\n",
      "        [49., 64., 81.]]) \n",
      "\n",
      "tensor([[ 6.,  9., 14.],\n",
      "        [21., 30., 41.],\n",
      "        [54., 69., 86.]])\n"
     ]
    }
   ],
   "source": [
    "# 打印tensor\n",
    "# 打印tensor\n",
    "print(tensor, \"\\n\")\n",
    "# 将tensor的值增加5\n",
    "tensor.add_(5)\n",
    "# tensor = tensor + 5\n",
    "# tensor += 5\n",
    "# 打印增加后的tensor\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.,  9., 14.],\n",
       "        [21., 30., 41.],\n",
       "        [54., 69., 86.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算图\n",
    "\n",
    "计算图\n",
    "在进⼀步学习pytorch之前，先要了解⼀个概念 —— 计算图( Computation graph)所有的深度学习框架都依赖于计算图来完成梯度下降、优化梯度值等计算。⽽计算图的创建和应⽤，通常包含如下两个部分：\n",
    "\n",
    "1. ⽤⼾构建前向传播图\n",
    "2. 框架处理后向传播(梯度更新)\n",
    "\n",
    "模型从简单到复杂，pytorch和tensorflow都使⽤计算图来完成⼯作。但是，这两个框架所使⽤的计算图也却有所不同：tensorflow1.x 使⽤的是静态计算图，tensorflow2.x和pytorch使⽤的是动态计算图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7056, -1.1294,  1.3409, -0.7608, -0.5293, -0.1568, -0.2279,  0.2818,\n",
      "          0.0207, -0.7942],\n",
      "        [ 0.7274, -1.2783,  0.2343, -0.1232,  0.1612,  1.1243,  0.5018,  1.5124,\n",
      "         -1.3377,  0.9328],\n",
      "        [-1.1371,  1.4808, -0.6457,  1.6646, -1.2049,  0.8178, -0.6517, -1.1324,\n",
      "         -1.3577,  0.4634],\n",
      "        [-0.1495,  1.0262, -1.2738,  0.0795,  2.0065,  1.0519, -0.4876, -1.3043,\n",
      "         -1.8095,  1.2450],\n",
      "        [-0.4401, -0.3208, -1.3851,  0.5539,  0.0972, -0.4738,  1.3937, -0.2813,\n",
      "          1.5669, -0.2977],\n",
      "        [-0.7179, -1.5290,  1.2114, -0.2846,  0.1133,  0.3436,  0.1981,  0.6191,\n",
      "         -0.4928,  0.9062],\n",
      "        [ 1.1626,  0.0691,  0.3070,  0.6794, -0.3059, -0.3886,  0.1550, -1.9580,\n",
      "          0.8805, -1.6631],\n",
      "        [ 0.3296, -0.9578, -0.0962,  1.0546, -0.9969, -0.6307, -0.4276, -1.4007,\n",
      "          0.8332, -0.8394],\n",
      "        [-0.3763, -1.0011, -0.6376,  1.1612, -0.8291, -0.4670,  0.5048,  2.6009,\n",
      "          0.1233,  0.2711],\n",
      "        [-0.1989, -0.9202, -0.1130, -0.2508,  0.9791, -0.6820,  0.9950,  0.0328,\n",
      "          0.3286, -1.9296]], requires_grad=True)\n",
      "tensor([ 0.1616,  0.8613, -1.5417, -2.3347, -1.1109,  0.4218, -0.1945,  0.6640,\n",
      "         0.3631,  0.4282], requires_grad=True)\n",
      "tensor([-1.4682], requires_grad=True)\n",
      "tensor([ 1.4711,  0.5265, -0.9446, -1.7636, -0.2005,  0.3670,  0.4678,  0.2059,\n",
      "        -0.9594, -2.8075], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'expression.png'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "\n",
    "# 定义矩阵 A，向量 b 和常数 c\n",
    "A = torch.randn(10, 10,requires_grad=True)  # requires_grad=True 表示我们要对 A 求导\n",
    "b = torch.randn(10,requires_grad=True)\n",
    "c = torch.randn(1,requires_grad=True)\n",
    "x = torch.randn(10, requires_grad=True)\n",
    "\n",
    "# # 不设置 requires_grad=True 这个参数\n",
    "# A = torch.randn(10, 10)  # requires_grad=True 表示我们要对 A 求导\n",
    "# b = torch.randn(10)\n",
    "# c = torch.randn(1)\n",
    "# x = torch.randn(10)\n",
    "\n",
    "print(A)\n",
    "print(b)\n",
    "print(c)\n",
    "print(x)\n",
    "\n",
    "# 计算 x^T * A + b * x + c\n",
    "result = torch.matmul(A, x.T) + torch.matmul(b, x) + c\n",
    "\n",
    "# 生成计算图节点\n",
    "dot = make_dot(result, params={'A': A, 'b': b, 'c': c, 'x': x})\n",
    "# 绘制计算图\n",
    "dot.render('expression', format='png', cleanup=True, view=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
